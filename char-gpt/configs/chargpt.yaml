model:
  vocab_size: 65
  n_embed: 384
  num_heads: 6
  n_layer: 6
  max_seq_length: 256
  dropout: 0.2

training:
  batch_size: 64 # per GPU
  sequence_length: 256
  learning_rate: 1e-4
  max_steps: 10000
  eval_interval: 100
  val_samples: 64
  device: "cuda" 
  seed: 42
  train_split: 0.9
  # Distributed training settings
  distributed: true
  world_size: 4
  # Logging settings
  wandb:
    project: "char-gpt"
    name: "default"
    log_interval: 100
  # Checkpointing settings
  checkpoint:
    save_interval: 250
    save_dir: "checkpoints"
    max_keep: 3  # Maximum number of checkpoints to keep